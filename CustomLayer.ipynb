{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomLayer 定義\n",
    "\n",
    "如果你想要自己根據論文公式刻出一個model，下面一有一個最基本的範例可供使用\n",
    "PyTorch 在定義model會需要用到Parameter來打包可訓練參數\n",
    "這樣在優化器指定要優化的參數時，才可以透過Module.parameter()這個函式來快速調參\n",
    "\n",
    "而要注意的是當資料是以Batch的形式呈現，計算時要注意batch的軸不能消失\n",
    "否則在訓練資料的過程中就會失去Batch Training的意義\n",
    "\n",
    "也就是說公式需要改寫為\n",
    "## $$y=xW^T+b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定義一個layer來做影像辨識\n",
    "class LinearLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_feature, out_feature) -> None:\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.w1 = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(size = (out_feature, in_feature), requires_grad=True)))\n",
    "        self.b1 = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(size = (1, out_feature))).squeeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x @ self.w1.T + self.b1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root='./basic_dataset', train = True, download= True, transform=ToTensor())\n",
    "test_dataset = MNIST(root='./basic_dataset', train = False, download= True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(train_dataset, batch_size=32, shuffle= True)\n",
    "testLoader = DataLoader(test_dataset, batch_size=32, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): LinearLayer()\n",
      "    (1): ReLU()\n",
      "    (2): LinearLayer()\n",
      "    (3): ReLU()\n",
      "    (4): LinearLayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            LinearLayer(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            LinearLayer(512, 512),\n",
    "            nn.ReLU(),\n",
    "            LinearLayer(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0514,  0.0014,  0.0230,  ...,  0.0055,  0.0263, -0.0310],\n",
      "        [-0.0379,  0.0376, -0.0229,  ...,  0.0488,  0.0039, -0.0314],\n",
      "        [ 0.0499,  0.0609, -0.0008,  ..., -0.0174,  0.0624, -0.0014],\n",
      "        ...,\n",
      "        [ 0.0024,  0.0220, -0.0107,  ...,  0.0538,  0.0397,  0.0126],\n",
      "        [-0.0637, -0.0604, -0.0328,  ...,  0.0672, -0.0631,  0.0134],\n",
      "        [ 0.0256,  0.0408,  0.0445,  ..., -0.0035,  0.0662,  0.0348]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 2.0173e-02, -1.9361e-02, -8.1276e-02,  8.9001e-02,  2.9033e-02,\n",
      "        -5.1355e-02,  2.3378e-02, -6.9840e-02,  8.6917e-03, -1.9112e-02,\n",
      "         6.6321e-02, -7.4989e-02, -5.0029e-02, -1.9164e-02,  7.4624e-02,\n",
      "         4.5056e-02,  8.3704e-02, -8.3460e-02,  3.4388e-02,  3.0743e-02,\n",
      "         1.0273e-01,  9.7634e-02,  6.7447e-02, -7.4686e-02,  5.0398e-02,\n",
      "         3.5076e-02,  3.1794e-02,  1.6450e-02, -6.7021e-02, -3.5462e-02,\n",
      "        -8.8568e-02, -9.9488e-02,  9.5412e-02,  8.3156e-02, -7.2863e-02,\n",
      "         1.0315e-01,  9.8573e-02, -6.1618e-02, -6.5296e-02,  4.4915e-02,\n",
      "         6.9568e-02, -1.2290e-02,  4.4671e-02,  7.6985e-02,  9.1216e-02,\n",
      "         9.7906e-02, -1.0203e-01,  9.6384e-02, -1.0459e-01,  7.7490e-02,\n",
      "        -6.7746e-02,  3.5754e-02,  1.0737e-01,  9.4901e-02,  8.7698e-02,\n",
      "        -6.3258e-02,  1.0453e-01, -1.0456e-01,  1.0303e-02,  9.0682e-02,\n",
      "        -9.0684e-02, -6.3558e-02,  9.7716e-02,  4.5149e-02, -4.3696e-02,\n",
      "        -1.7464e-02,  2.2252e-02,  9.8124e-02,  1.0469e-01, -9.8263e-02,\n",
      "         3.8757e-02, -9.9853e-02,  4.1184e-02, -6.1183e-02, -1.4401e-02,\n",
      "        -4.7140e-02,  6.8754e-02, -3.2430e-02, -3.8778e-02, -4.0667e-02,\n",
      "         4.9045e-02,  4.8671e-02, -1.7646e-03, -3.2292e-02, -7.7899e-03,\n",
      "        -7.4891e-02, -5.7375e-02,  6.5527e-02, -3.6188e-02,  3.6242e-03,\n",
      "        -1.0642e-01, -8.4818e-03,  1.0684e-02, -6.2654e-02,  3.4888e-02,\n",
      "         1.6474e-02, -6.6789e-02, -8.8147e-02, -8.8793e-02,  6.6742e-02,\n",
      "         9.3869e-02, -1.9409e-02, -4.5920e-03, -5.7345e-02,  7.7520e-02,\n",
      "         8.9811e-02, -7.7099e-02, -1.6719e-03, -3.7458e-02, -1.6354e-03,\n",
      "         4.4808e-02, -6.9879e-02, -5.0119e-02, -2.2423e-02, -8.6086e-02,\n",
      "         5.2380e-02, -5.4474e-02,  3.3502e-02, -6.0277e-02, -2.0331e-04,\n",
      "        -2.3307e-02, -9.0207e-02,  4.0760e-02,  7.6131e-03,  1.0446e-01,\n",
      "         8.5733e-02,  9.3978e-02, -2.8459e-02, -3.9836e-02,  9.0746e-02,\n",
      "         8.5053e-02,  4.6138e-02,  1.7795e-02,  8.4467e-02,  6.9918e-02,\n",
      "         4.1330e-02, -2.4515e-02,  1.1743e-02,  3.9028e-03, -7.5203e-02,\n",
      "        -9.8581e-02, -7.6033e-02,  5.5034e-02, -3.3750e-03,  5.0343e-02,\n",
      "         1.6507e-02, -9.8789e-02, -8.0531e-02,  9.9130e-05, -6.7847e-02,\n",
      "         6.1843e-02,  3.3215e-02, -9.5940e-02, -5.4955e-03, -1.4034e-02,\n",
      "        -3.9942e-02,  9.5733e-03,  3.1720e-02, -9.9500e-02, -6.5896e-02,\n",
      "         7.8410e-02, -8.7143e-02, -1.0271e-01, -4.1683e-02,  7.5740e-02,\n",
      "        -1.0004e-01,  6.8712e-02,  5.4453e-02, -5.5231e-02,  8.2757e-02,\n",
      "         9.7688e-02,  1.0774e-01,  2.3224e-02, -1.6216e-02,  1.7353e-02,\n",
      "        -1.0620e-01,  8.0657e-02, -3.7830e-02,  3.1883e-02, -9.5167e-02,\n",
      "        -8.8980e-02, -5.9199e-02,  4.7094e-02, -1.7763e-02,  4.0884e-02,\n",
      "         2.1651e-02, -2.0682e-02,  3.2494e-02,  3.1749e-02,  4.9859e-02,\n",
      "        -8.1228e-02,  1.4549e-02, -3.0206e-02,  1.0386e-01,  5.9201e-02,\n",
      "         2.7394e-02,  5.7959e-02,  5.7659e-02,  5.5423e-02, -3.6048e-02,\n",
      "         9.1384e-02,  1.0705e-01, -5.8596e-03, -7.0561e-02, -3.2169e-02,\n",
      "        -1.5003e-02,  4.4217e-02,  3.5197e-02,  1.1842e-02, -1.0541e-01,\n",
      "        -6.3858e-02, -9.0314e-02, -1.0520e-01,  6.2707e-02, -5.9227e-02,\n",
      "        -6.3138e-02,  3.6281e-03, -6.8296e-02,  2.9048e-03,  8.6073e-02,\n",
      "         1.9329e-02, -3.4369e-02, -3.1937e-02, -6.0628e-02,  1.8461e-02,\n",
      "         1.0436e-02,  9.8413e-02, -5.5508e-02, -9.7553e-02,  6.1487e-02,\n",
      "         1.3891e-03,  9.4756e-02, -9.3281e-02,  6.4682e-02,  2.5201e-02,\n",
      "         8.2745e-02, -3.8480e-02, -9.9303e-02,  3.4782e-02, -4.8623e-02,\n",
      "         1.5384e-02, -4.7482e-02, -9.3606e-02, -1.0971e-02, -8.3103e-02,\n",
      "        -1.0772e-01, -1.0785e-01,  6.7011e-02, -4.4726e-02, -8.1254e-02,\n",
      "        -1.0973e-02,  5.1174e-02, -4.4185e-02, -6.1418e-02, -4.6305e-05,\n",
      "        -5.3028e-02,  8.9554e-03, -1.0564e-02, -9.0394e-02, -6.6478e-02,\n",
      "        -5.7625e-02, -1.0207e-01, -1.0212e-01,  7.9253e-02, -9.8638e-02,\n",
      "        -9.6216e-03,  3.8240e-02, -8.6355e-02,  8.5679e-02, -1.0642e-01,\n",
      "         3.8642e-02,  9.7685e-02,  9.8271e-02,  5.4762e-02,  3.9825e-02,\n",
      "        -2.4433e-02, -8.4781e-02, -1.8580e-02, -8.9036e-03,  5.1531e-03,\n",
      "        -4.0563e-02,  1.0224e-01,  5.6621e-02,  1.0216e-01,  5.5510e-02,\n",
      "        -5.6988e-02, -3.4677e-02, -9.1110e-02, -1.6488e-03, -1.0574e-01,\n",
      "         2.8106e-02,  7.9673e-02, -2.6254e-02,  3.3248e-02,  8.8865e-02,\n",
      "        -9.8142e-02, -1.0479e-01, -4.6525e-02,  1.1030e-02,  8.3954e-02,\n",
      "         4.1215e-02, -1.0026e-01, -6.0677e-02, -4.6247e-02, -7.9703e-02,\n",
      "        -3.9882e-02,  3.0891e-02, -2.8715e-03,  3.8380e-03,  6.1074e-02,\n",
      "         6.3472e-02, -6.3425e-02,  1.5900e-02,  7.1875e-03,  9.5932e-02,\n",
      "         3.5739e-02, -1.2208e-02, -9.7025e-02,  4.0622e-02,  4.7207e-03,\n",
      "        -9.9156e-03,  1.0512e-01,  2.2444e-02, -9.7287e-02,  6.3710e-02,\n",
      "         5.9092e-03, -4.4312e-02,  7.6647e-02, -2.5829e-02,  9.4874e-02,\n",
      "        -6.8418e-02, -8.8189e-02,  4.9553e-02, -9.0474e-02,  4.0363e-02,\n",
      "        -6.4829e-02,  8.7185e-02,  4.2322e-02,  7.8061e-02,  2.0019e-02,\n",
      "        -5.4177e-02,  1.3563e-02, -1.0090e-01,  7.3733e-02, -4.2334e-03,\n",
      "        -9.5460e-02, -6.6963e-02, -4.1263e-02,  4.6613e-02,  2.8055e-02,\n",
      "         3.5260e-02,  1.0721e-01, -2.9136e-02, -3.4974e-03, -2.0868e-02,\n",
      "        -5.4409e-02,  6.9911e-02,  8.1541e-02, -5.1242e-02,  2.0358e-02,\n",
      "        -2.1306e-02,  3.6002e-02,  2.6097e-02,  2.4172e-02,  4.3205e-02,\n",
      "         9.0414e-02, -8.2785e-02, -1.0428e-01,  2.1522e-02,  8.5639e-02,\n",
      "        -6.9074e-02,  9.1125e-02,  3.2028e-02, -2.1113e-02,  2.7872e-02,\n",
      "        -6.7044e-02,  8.9030e-02, -1.0076e-01, -3.9374e-02, -5.0923e-02,\n",
      "        -3.9049e-02, -3.6786e-03,  8.2980e-02, -1.0329e-01, -2.7734e-02,\n",
      "         1.9175e-02,  8.2284e-02,  4.6282e-02, -6.3265e-02,  7.6772e-02,\n",
      "         4.3313e-02,  7.0513e-03,  9.4452e-02,  2.2440e-03,  1.4796e-02,\n",
      "        -1.3284e-02,  2.5528e-02, -1.0564e-01, -8.9994e-02, -9.8567e-02,\n",
      "        -5.8204e-02, -3.6287e-02,  3.2099e-02, -7.3230e-02,  1.5715e-03,\n",
      "        -7.2948e-03,  8.9742e-02,  1.0475e-02, -8.9012e-02, -5.3699e-03,\n",
      "         2.4667e-02, -6.6098e-02,  9.9541e-02,  8.4759e-02,  3.8867e-03,\n",
      "        -1.9811e-02,  1.0040e-01, -7.9226e-02,  5.7044e-02,  3.4409e-02,\n",
      "        -8.6461e-02,  9.7428e-02, -1.0514e-01, -6.4684e-02, -4.7474e-02,\n",
      "         1.0348e-02, -4.2014e-02, -4.1738e-02, -4.1411e-02, -6.1270e-02,\n",
      "         5.7551e-02,  3.4919e-02, -9.3693e-02, -1.6977e-02,  2.1621e-02,\n",
      "        -5.7319e-02,  4.4631e-03,  1.2279e-02,  1.2032e-02,  1.9211e-03,\n",
      "        -3.8421e-02,  4.8149e-02, -7.2232e-02, -1.0414e-01, -9.1383e-02,\n",
      "         6.8336e-02,  9.6538e-02,  1.8810e-02,  3.4420e-02,  3.7159e-02,\n",
      "        -5.6851e-02,  6.0250e-02, -4.2456e-03, -7.7378e-02, -9.2426e-02,\n",
      "        -6.0222e-02, -7.3293e-02, -7.6493e-02,  3.1952e-02,  4.1164e-02,\n",
      "        -4.9264e-02,  4.8078e-02, -3.7312e-03, -1.3407e-02,  2.1498e-02,\n",
      "         9.1569e-02,  3.9437e-02,  9.8412e-02, -9.8476e-02, -4.3413e-02,\n",
      "        -3.2869e-02,  1.0783e-01,  7.6040e-02, -4.7179e-02, -7.4324e-02,\n",
      "        -6.5489e-02, -7.4133e-02,  8.6453e-02, -9.5155e-02,  5.6636e-02,\n",
      "        -1.0012e-01, -7.7930e-03,  9.6778e-02, -5.6562e-02, -1.6209e-02,\n",
      "        -5.0827e-02,  6.3266e-02,  6.8844e-02,  5.2528e-02, -8.3354e-02,\n",
      "         2.0316e-02,  2.4533e-03, -2.5661e-02, -6.4338e-02, -3.5975e-02,\n",
      "         5.0067e-03, -3.4383e-02, -3.1892e-02, -7.2334e-02,  7.5827e-03,\n",
      "        -3.7121e-02,  2.4626e-03,  1.1623e-02, -4.1542e-02, -5.7741e-02,\n",
      "        -2.2112e-02, -1.0274e-01,  8.3202e-02,  1.6102e-02,  3.7198e-02,\n",
      "        -8.6011e-02,  3.2511e-02], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0598, -0.0754, -0.0255,  ..., -0.0205,  0.0030, -0.0199],\n",
      "        [-0.0680, -0.0180, -0.0257,  ..., -0.0601,  0.0251,  0.0722],\n",
      "        [ 0.0533, -0.0355, -0.0510,  ...,  0.0332, -0.0013, -0.0246],\n",
      "        ...,\n",
      "        [-0.0605, -0.0023,  0.0204,  ..., -0.0478, -0.0352, -0.0683],\n",
      "        [-0.0508,  0.0516,  0.0241,  ..., -0.0387,  0.0731,  0.0372],\n",
      "        [ 0.0358, -0.0722, -0.0122,  ...,  0.0576, -0.0325,  0.0496]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-4.5893e-02,  1.0310e-01,  8.7205e-02, -9.1374e-02,  6.1497e-02,\n",
      "        -3.8719e-03, -9.9839e-02, -4.5419e-02,  7.7722e-02,  3.4602e-03,\n",
      "         1.0620e-01, -1.0718e-01, -8.9169e-02,  1.1907e-02,  3.9807e-02,\n",
      "         7.4142e-02, -7.2834e-03,  3.9534e-02,  3.5764e-02,  1.6973e-03,\n",
      "        -3.5611e-02,  2.7221e-02, -7.7346e-02,  1.0801e-01,  7.0190e-02,\n",
      "         3.2167e-02,  1.2122e-02, -2.6769e-02, -7.0343e-02, -4.1272e-02,\n",
      "         9.2132e-02,  5.4486e-03, -2.4903e-02, -8.0634e-02,  7.9943e-02,\n",
      "        -4.9743e-02,  8.8750e-02,  6.8281e-02,  1.4107e-02, -1.0372e-01,\n",
      "        -8.3565e-03,  4.3626e-03, -8.3157e-02, -1.0241e-01, -6.7837e-02,\n",
      "        -3.5901e-02,  1.0819e-03,  9.4387e-02, -9.3433e-02,  6.4256e-02,\n",
      "        -5.2386e-03, -8.3935e-02,  1.2224e-02,  4.8957e-02, -1.0108e-01,\n",
      "        -3.1999e-02, -9.3409e-03, -9.8974e-02, -1.0441e-01, -8.9272e-02,\n",
      "        -6.3964e-02,  1.6334e-02,  5.2773e-02,  4.4923e-02, -2.8717e-02,\n",
      "        -9.2984e-02, -4.3150e-02, -1.9070e-02,  1.0652e-01,  5.4003e-02,\n",
      "         8.3218e-02,  3.6106e-02,  1.0622e-01, -5.2782e-02, -4.6565e-02,\n",
      "        -3.9256e-02,  1.4189e-02,  9.2333e-03,  8.6079e-02, -3.2856e-02,\n",
      "        -2.0233e-02,  1.2812e-02,  4.3019e-03, -5.0225e-02,  1.0366e-01,\n",
      "        -6.1587e-03,  7.8372e-02,  7.1751e-03,  5.7207e-02,  1.6443e-02,\n",
      "        -1.9908e-02, -2.8805e-02,  1.7169e-02,  1.0590e-01,  3.2402e-02,\n",
      "         5.1482e-02,  4.7109e-03,  1.0400e-01,  6.1177e-02, -1.1359e-02,\n",
      "        -1.0210e-01, -7.2908e-02,  4.1220e-03, -7.6137e-02,  4.5192e-02,\n",
      "        -9.3309e-02, -1.2377e-02,  4.2656e-02, -4.9277e-03,  9.2583e-02,\n",
      "        -5.7872e-02,  1.9795e-02, -1.9349e-05,  3.3371e-02,  4.1327e-02,\n",
      "        -9.0026e-02, -8.4759e-02,  8.7552e-02, -1.0093e-01,  3.6446e-02,\n",
      "        -1.1906e-02, -8.2193e-02,  6.7170e-02, -8.5699e-02,  4.3276e-02,\n",
      "        -6.4823e-02, -1.0658e-01, -4.2333e-02, -8.4132e-02,  3.3322e-02,\n",
      "         2.3865e-02, -2.8886e-02,  9.2485e-02,  6.5857e-02, -1.2122e-02,\n",
      "         9.2827e-02,  6.5674e-02, -8.5619e-02, -8.5985e-02,  6.9999e-02,\n",
      "        -9.8204e-02, -3.0206e-02,  5.0682e-02, -3.2424e-02, -5.9418e-02,\n",
      "        -1.4961e-02,  1.5014e-02,  8.6234e-02,  5.6765e-02, -9.6558e-03,\n",
      "        -4.9492e-02,  1.0087e-01, -6.0074e-02,  8.5081e-02, -1.3360e-02,\n",
      "        -7.0487e-02,  6.5777e-02,  8.0448e-02, -9.1131e-02, -1.8696e-02,\n",
      "         9.0287e-02, -9.5609e-02,  8.7811e-03,  9.8770e-02,  4.3929e-02,\n",
      "         8.8738e-02,  1.0439e-01,  5.7606e-02,  5.0114e-02,  1.5147e-04,\n",
      "         4.2052e-02, -1.0365e-01, -1.5557e-02,  8.5952e-02,  3.0164e-02,\n",
      "        -6.5096e-02, -4.2067e-02, -8.7329e-02, -9.0459e-02, -5.5928e-02,\n",
      "        -4.1969e-02,  3.1899e-02,  8.2188e-03, -8.7040e-02,  1.0693e-01,\n",
      "         8.9529e-02,  8.9739e-02, -4.6097e-02,  8.6652e-02,  1.4118e-02,\n",
      "         1.0047e-02, -1.1341e-02, -6.7879e-02, -2.0305e-03,  1.0472e-01,\n",
      "        -1.0535e-01, -7.1121e-02, -1.5204e-02, -6.1032e-02,  1.0431e-01,\n",
      "         5.4124e-02, -8.2398e-02, -4.5147e-02,  6.0506e-02,  2.4050e-02,\n",
      "         4.3707e-02, -9.5763e-02, -8.9202e-02, -1.8310e-02, -6.5529e-02,\n",
      "         8.7015e-02,  6.0394e-02, -1.6480e-02,  6.4174e-02, -7.4976e-04,\n",
      "        -9.1144e-02, -4.6161e-02,  3.1786e-02,  7.8248e-02,  3.9046e-02,\n",
      "         2.5117e-02,  1.3858e-02,  7.8228e-02, -4.0706e-02, -6.4319e-02,\n",
      "        -9.2797e-02, -4.4876e-02, -5.7502e-02, -3.5341e-02, -7.2098e-02,\n",
      "         5.6844e-03,  1.0014e-01,  6.5911e-02,  6.5424e-02, -8.1847e-02,\n",
      "        -1.0462e-02, -7.1432e-02,  1.3778e-02, -4.0225e-02,  3.6490e-02,\n",
      "        -2.8662e-02, -6.8035e-02,  7.2230e-02,  9.7276e-02, -5.0943e-02,\n",
      "        -1.0700e-01, -4.9870e-02,  7.6862e-02,  6.9807e-02,  4.4618e-03,\n",
      "         1.2706e-02,  3.8681e-02, -3.9923e-02,  5.7128e-02, -1.0228e-02,\n",
      "        -4.6786e-02, -7.3404e-02,  1.0313e-01,  1.8315e-02, -5.4064e-02,\n",
      "         7.8586e-03,  8.9570e-02, -8.8738e-02, -5.5119e-02, -4.5880e-02,\n",
      "        -5.3344e-02,  9.2031e-02,  4.1057e-02,  4.6865e-02, -4.6330e-02,\n",
      "         1.5944e-02, -9.4806e-02, -5.0022e-02, -9.0266e-02,  1.0242e-01,\n",
      "         6.1399e-02, -3.5962e-02, -1.0538e-01,  2.5620e-02, -5.5552e-03,\n",
      "        -4.1307e-02, -1.5737e-02, -1.6118e-02,  9.7271e-02, -5.3197e-02,\n",
      "         8.1500e-02,  9.2324e-02,  3.5660e-02,  2.9710e-03, -9.5489e-02,\n",
      "        -7.3083e-02,  8.7928e-03,  8.7799e-02,  9.9266e-02,  1.0059e-01,\n",
      "        -6.4001e-02, -2.0670e-03, -9.5564e-03,  8.0480e-02,  8.9078e-02,\n",
      "        -2.5574e-02,  3.1773e-02, -3.0880e-02,  7.6100e-02, -3.5415e-02,\n",
      "         1.0094e-01,  1.9463e-03,  4.6177e-02,  5.6334e-02, -1.0686e-01,\n",
      "        -9.6303e-02, -5.9478e-03,  5.3051e-02,  4.5426e-02, -9.0105e-03,\n",
      "        -3.5042e-02,  9.2436e-02,  1.3640e-02,  9.9151e-02, -1.1561e-02,\n",
      "        -1.0596e-01, -1.9795e-02,  3.1814e-02,  2.1896e-02,  5.8430e-03,\n",
      "        -5.9985e-02,  9.5171e-02,  7.9260e-02,  3.4800e-02, -7.4937e-02,\n",
      "         8.3493e-02,  5.5230e-02, -2.8247e-02, -4.6870e-02, -4.1209e-02,\n",
      "        -9.3719e-02,  6.5200e-02,  5.3461e-02, -4.5135e-02, -5.2951e-02,\n",
      "         1.3953e-02,  7.1316e-02, -1.7673e-02, -2.8502e-02,  6.2986e-02,\n",
      "         4.8826e-02,  1.4405e-02,  3.3364e-02, -5.9685e-02, -8.4464e-02,\n",
      "        -5.0154e-02, -2.1358e-02, -3.6843e-02,  2.8910e-02, -3.3210e-02,\n",
      "         2.6110e-02,  4.1470e-02,  6.5957e-02, -1.0785e-01, -2.3443e-02,\n",
      "         3.7561e-02, -3.1792e-02, -3.0294e-02, -4.8113e-02, -7.8979e-02,\n",
      "         3.5714e-02,  7.5970e-02,  3.8044e-02,  8.1805e-02, -5.4673e-02,\n",
      "        -1.2344e-02,  1.0224e-01, -1.0267e-01, -1.8271e-03, -8.4172e-02,\n",
      "        -5.7032e-02, -6.8928e-02,  9.2164e-03,  2.9618e-02, -1.0400e-01,\n",
      "         6.9761e-02, -7.5031e-02,  7.9557e-02, -4.2341e-02,  3.6104e-03,\n",
      "         9.6292e-02, -7.3850e-02,  7.3606e-03, -7.6153e-02, -2.5551e-02,\n",
      "        -4.1967e-02, -9.2013e-02,  8.0805e-02,  4.4568e-02, -1.0232e-01,\n",
      "         4.2388e-02,  9.4259e-02, -1.0757e-01,  4.4491e-02, -8.5580e-02,\n",
      "        -5.7441e-02, -1.8007e-02, -4.3423e-02,  8.0578e-02,  9.4304e-02,\n",
      "         3.0314e-02, -4.8691e-02,  2.8632e-02,  3.5735e-02, -2.4079e-02,\n",
      "        -1.6669e-02, -8.6176e-02, -9.6146e-02,  7.0038e-02,  8.5268e-02,\n",
      "         5.2879e-02,  5.3066e-03,  7.9947e-02, -3.4028e-02, -9.0876e-03,\n",
      "         6.6742e-02,  1.0533e-01, -6.4927e-02,  1.9780e-02, -5.4596e-02,\n",
      "         6.9418e-02,  8.9311e-02,  7.5508e-02,  3.7077e-02,  2.7574e-02,\n",
      "         4.8747e-02,  8.6464e-02,  9.2396e-02,  8.4475e-02,  7.2969e-02,\n",
      "         1.9911e-02,  9.7498e-02, -1.0214e-01, -8.9186e-02,  1.0485e-02,\n",
      "        -8.4702e-02, -9.8435e-02,  1.8224e-03,  8.4578e-02, -8.6657e-02,\n",
      "        -7.7668e-02, -3.8412e-02, -4.8175e-02, -9.9745e-02,  9.4930e-02,\n",
      "         3.7600e-02,  4.1352e-02, -1.0354e-02, -1.4347e-02,  4.0993e-02,\n",
      "         9.9439e-02, -6.7302e-02,  5.0515e-02,  7.2877e-02,  6.3799e-02,\n",
      "         4.1529e-02, -7.8766e-02, -8.8815e-02,  2.7898e-02,  2.1667e-02,\n",
      "        -3.0292e-02,  6.6528e-02, -1.0099e-01, -9.4585e-02, -4.5322e-02,\n",
      "        -4.8185e-02,  1.6433e-02, -8.4136e-02, -5.9200e-02,  3.5005e-03,\n",
      "        -8.2186e-02,  1.6047e-03, -6.7181e-02, -2.6152e-02,  4.4273e-02,\n",
      "        -7.3503e-02,  1.1343e-02, -8.0284e-03, -2.8849e-02,  9.0666e-02,\n",
      "         6.5846e-02, -9.7132e-02, -7.0489e-02,  1.5104e-02, -2.5054e-02,\n",
      "        -5.5314e-02, -1.0947e-02,  8.1447e-02,  1.1482e-03,  1.9236e-02,\n",
      "        -1.0142e-01, -1.6925e-02, -6.6907e-02, -7.2697e-02, -8.4703e-02,\n",
      "         1.8297e-02, -1.3855e-02, -5.9828e-02, -5.8965e-02, -1.2969e-02,\n",
      "        -1.5411e-02, -6.2843e-02,  3.0321e-02, -5.3710e-02,  3.1320e-02,\n",
      "         6.0356e-02,  1.3030e-02], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0629,  0.0919, -0.0053,  ...,  0.0823,  0.0231, -0.0868],\n",
      "        [-0.0319, -0.0350, -0.0450,  ..., -0.0820, -0.0133, -0.0259],\n",
      "        [-0.0876, -0.0206,  0.0430,  ..., -0.0310,  0.0538,  0.0913],\n",
      "        ...,\n",
      "        [-0.0237, -0.0853,  0.0233,  ..., -0.0126, -0.0914,  0.0349],\n",
      "        [ 0.0045, -0.0276, -0.0017,  ..., -0.0373, -0.0759,  0.0996],\n",
      "        [-0.0822, -0.0132, -0.0999,  ..., -0.0448,  0.0821, -0.0154]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6531,  0.2418, -0.4260, -0.6602,  0.6769, -0.1422,  0.4207,  0.3878,\n",
      "        -0.5069, -0.1704], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for parm in model.parameters():\n",
    "    print(parm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.442665  [    0/60000]\n",
      "loss: 2.236519  [ 3200/60000]\n",
      "loss: 2.206225  [ 6400/60000]\n",
      "loss: 2.168396  [ 9600/60000]\n",
      "loss: 2.124247  [12800/60000]\n",
      "loss: 1.971074  [16000/60000]\n",
      "loss: 1.983765  [19200/60000]\n",
      "loss: 1.875924  [22400/60000]\n",
      "loss: 1.853083  [25600/60000]\n",
      "loss: 1.666651  [28800/60000]\n",
      "loss: 1.629444  [32000/60000]\n",
      "loss: 1.580033  [35200/60000]\n",
      "loss: 1.502506  [38400/60000]\n",
      "loss: 1.510396  [41600/60000]\n",
      "loss: 1.278157  [44800/60000]\n",
      "loss: 1.287291  [48000/60000]\n",
      "loss: 1.135813  [51200/60000]\n",
      "loss: 1.258750  [54400/60000]\n",
      "loss: 1.095076  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 1.074352 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.992744  [    0/60000]\n",
      "loss: 0.968783  [ 3200/60000]\n",
      "loss: 0.944089  [ 6400/60000]\n",
      "loss: 1.056798  [ 9600/60000]\n",
      "loss: 1.049953  [12800/60000]\n",
      "loss: 1.074222  [16000/60000]\n",
      "loss: 0.626335  [19200/60000]\n",
      "loss: 0.670370  [22400/60000]\n",
      "loss: 0.833117  [25600/60000]\n",
      "loss: 0.715325  [28800/60000]\n",
      "loss: 0.816140  [32000/60000]\n",
      "loss: 0.567422  [35200/60000]\n",
      "loss: 0.790929  [38400/60000]\n",
      "loss: 0.712161  [41600/60000]\n",
      "loss: 0.816020  [44800/60000]\n",
      "loss: 0.651458  [48000/60000]\n",
      "loss: 0.667623  [51200/60000]\n",
      "loss: 0.699828  [54400/60000]\n",
      "loss: 0.774963  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.612811 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.549452  [    0/60000]\n",
      "loss: 0.655762  [ 3200/60000]\n",
      "loss: 0.637030  [ 6400/60000]\n",
      "loss: 0.483626  [ 9600/60000]\n",
      "loss: 0.517245  [12800/60000]\n",
      "loss: 0.652844  [16000/60000]\n",
      "loss: 0.695683  [19200/60000]\n",
      "loss: 0.631909  [22400/60000]\n",
      "loss: 0.618763  [25600/60000]\n",
      "loss: 0.600786  [28800/60000]\n",
      "loss: 0.411337  [32000/60000]\n",
      "loss: 0.711435  [35200/60000]\n",
      "loss: 0.340820  [38400/60000]\n",
      "loss: 0.547009  [41600/60000]\n",
      "loss: 0.548311  [44800/60000]\n",
      "loss: 0.314522  [48000/60000]\n",
      "loss: 0.415864  [51200/60000]\n",
      "loss: 0.572247  [54400/60000]\n",
      "loss: 0.432891  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.474006 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.318121  [    0/60000]\n",
      "loss: 0.453923  [ 3200/60000]\n",
      "loss: 0.361172  [ 6400/60000]\n",
      "loss: 0.348675  [ 9600/60000]\n",
      "loss: 0.427273  [12800/60000]\n",
      "loss: 0.863440  [16000/60000]\n",
      "loss: 0.512784  [19200/60000]\n",
      "loss: 0.505030  [22400/60000]\n",
      "loss: 0.409298  [25600/60000]\n",
      "loss: 0.371432  [28800/60000]\n",
      "loss: 0.496479  [32000/60000]\n",
      "loss: 0.465072  [35200/60000]\n",
      "loss: 0.454506  [38400/60000]\n",
      "loss: 0.346733  [41600/60000]\n",
      "loss: 0.612106  [44800/60000]\n",
      "loss: 0.433205  [48000/60000]\n",
      "loss: 0.356035  [51200/60000]\n",
      "loss: 0.334816  [54400/60000]\n",
      "loss: 0.392729  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.409729 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.317172  [    0/60000]\n",
      "loss: 0.232800  [ 3200/60000]\n",
      "loss: 0.392631  [ 6400/60000]\n",
      "loss: 0.438520  [ 9600/60000]\n",
      "loss: 0.624426  [12800/60000]\n",
      "loss: 0.447744  [16000/60000]\n",
      "loss: 0.570311  [19200/60000]\n",
      "loss: 0.277580  [22400/60000]\n",
      "loss: 0.520991  [25600/60000]\n",
      "loss: 0.281574  [28800/60000]\n",
      "loss: 0.490640  [32000/60000]\n",
      "loss: 0.446856  [35200/60000]\n",
      "loss: 0.573655  [38400/60000]\n",
      "loss: 0.213232  [41600/60000]\n",
      "loss: 0.424690  [44800/60000]\n",
      "loss: 0.190490  [48000/60000]\n",
      "loss: 0.450099  [51200/60000]\n",
      "loss: 0.284692  [54400/60000]\n",
      "loss: 0.661466  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.371123 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(trainLoader, model, loss_fn, optimizer)\n",
    "    test(testLoader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('torchEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "856971fa0a169429db2bf0c06b69517a5be6dea1d062d3d5bb82efdd1c5fddc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
