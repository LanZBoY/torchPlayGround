{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wen2Tee5\\Desktop\\Postgraduate\\torchPlayGround\\torchEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from NLPUtils.preprocessUtils import removePunctuation, removeWord\n",
    "from NLPUtils.DataModel import Vocabulary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '.\\\\R8'\n",
    "TRAIN = 'train.txt'\n",
    "TEST = 'test.txt'\n",
    "MAXLEN = 30\n",
    "TOKEN = {'<PAD>' : 0, '<UNK>' : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWordList = stopwords.words('english')\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(ROOT, TRAIN), encoding='utf-8',sep='\\t', header=None)\n",
    "df_test = pd.read_csv(os.path.join(ROOT, TRAIN), encoding='utf-8', sep = '\\t', header=None)\n",
    "label2idx = {label : i for i, label in enumerate(df_train[0].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = Vocabulary(TOKENS=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = []\n",
    "test_sentence = []\n",
    "for sentence in df_train[1]:\n",
    "    sentence = removePunctuation(sentence=sentence)\n",
    "    sentence = removeWord(removeWordList=removeWordList, sentence=sentence)\n",
    "    train_sentence.append(sentence)\n",
    "for sentence in df_test[1]:\n",
    "    sentence = removePunctuation(sentence=sentence)\n",
    "    sentence = removeWord(removeWordList=removeWordList, sentence=sentence)\n",
    "    test_sentence.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算tf-idf權重並建立詞彙庫\n",
    "vector = CountVectorizer(token_pattern=r'\\S+')\n",
    "table = vector.fit_transform(train_sentence)\n",
    "tfidf = TfidfTransformer()\n",
    "tfidfTable = tfidf.fit_transform(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = Vocabulary(TOKENS=TOKEN)\n",
    "voc.addWordList(vector.get_feature_names_out().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將資料encode為idx\n",
    "train_idxs = []\n",
    "for sentence in train_sentence:\n",
    "    data = sentence.split()[:MAXLEN]\n",
    "    train_idx = [voc.word2idx[word] for word in data]\n",
    "    while len(train_idx) < MAXLEN:\n",
    "        train_idx.append(voc.word2idx['<PAD>'])\n",
    "    train_idxs.append(train_idx)\n",
    "\n",
    "test_idxs = []\n",
    "for sentence in test_sentence:\n",
    "    data = sentence.split()[:MAXLEN]\n",
    "    test_idx = []\n",
    "    for word in data:\n",
    "        if voc.has(word):\n",
    "            test_idx.append(voc.word2idx[word])\n",
    "        else:\n",
    "            test_idx.append(voc.word2idx['<UNK>'])\n",
    "    while len(test_idx) < MAXLEN:\n",
    "        test_idx.append(voc.word2idx['<PAD>'])\n",
    "    test_idxs.append(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train_idxs, dtype=np.int64)\n",
    "x_test = np.array(test_idxs, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPMI\n",
    "WINDOW_SIZE = 3\n",
    "windows_list = []\n",
    "for sentence in x_train:\n",
    "    windows = [sentence[i : i + WINDOW_SIZE] for i in range(len(sentence) - WINDOW_SIZE + 1)]\n",
    "    windows_list += windows\n",
    "windows_list = np.array(windows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(windows_list == 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('torchEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "856971fa0a169429db2bf0c06b69517a5be6dea1d062d3d5bb82efdd1c5fddc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
