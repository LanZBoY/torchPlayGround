{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wen2Tee5\\Desktop\\Postgraduate\\torchPlayGround\\torchEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from NLPUtils.preprocessUtils import removePunctuation, removeWord\n",
    "from NLPUtils.DataModel import Vocabulary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '.\\\\R8'\n",
    "TRAIN = 'train.txt'\n",
    "TEST = 'test.txt'\n",
    "MAXLEN = 30\n",
    "TOKEN = {'<PAD>' : 0, '<UNK>' : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWordList = stopwords.words('english')\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(ROOT, TRAIN), encoding='utf-8',sep='\\t', header=None)\n",
    "df_test = pd.read_csv(os.path.join(ROOT, TRAIN), encoding='utf-8', sep = '\\t', header=None)\n",
    "label2idx = {label : i for i, label in enumerate(df_train[0].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = Vocabulary(TOKENS=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = []\n",
    "test_sentence = []\n",
    "for sentence in df_train[1]:\n",
    "    sentence = removePunctuation(sentence=sentence)\n",
    "    sentence = removeWord(removeWordList=removeWordList, sentence=sentence)\n",
    "    train_sentence.append(sentence)\n",
    "for sentence in df_test[1]:\n",
    "    sentence = removePunctuation(sentence=sentence)\n",
    "    sentence = removeWord(removeWordList=removeWordList, sentence=sentence)\n",
    "    test_sentence.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算tf-idf權重並建立詞彙庫\n",
    "vector = CountVectorizer(token_pattern=r'\\S+')\n",
    "table = vector.fit_transform(train_sentence)\n",
    "tfidf = TfidfTransformer()\n",
    "tfidfTable = tfidf.fit_transform(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = Vocabulary(TOKENS=TOKEN)\n",
    "voc.addWordList(vector.get_feature_names_out().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將資料encode為idx\n",
    "train_idxs = []\n",
    "for sentence in train_sentence:\n",
    "    data = sentence.split()[:MAXLEN]\n",
    "    train_idx = [voc.word2idx[word] for word in data]\n",
    "    while len(train_idx) < MAXLEN:\n",
    "        train_idx.append(voc.word2idx['<PAD>'])\n",
    "    train_idxs.append(train_idx)\n",
    "\n",
    "test_idxs = []\n",
    "for sentence in test_sentence:\n",
    "    data = sentence.split()[:MAXLEN]\n",
    "    test_idx = []\n",
    "    for word in data:\n",
    "        if voc.has(word):\n",
    "            test_idx.append(voc.word2idx[word])\n",
    "        else:\n",
    "            test_idx.append(voc.word2idx['<UNK>'])\n",
    "    while len(test_idx) < MAXLEN:\n",
    "        test_idx.append(voc.word2idx['<PAD>'])\n",
    "    test_idxs.append(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train_idxs, dtype=np.int64)\n",
    "x_test = np.array(test_idxs, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC = 'doc'\n",
    "WORD = 'word'\n",
    "IN  = 'in'\n",
    "OCCUR = 'occur'\n",
    "PRESERVE = 'preserve'\n",
    "DOC_PRESERVE = (DOC, PRESERVE, DOC)\n",
    "DOC_CONTAIN = (WORD, IN, DOC)\n",
    "WORD_OCCUR = (WORD, OCCUR, WORD)\n",
    "DIC_OFFSET = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立好初始化的圖\n",
    "graph_data = {\n",
    "    DOC_CONTAIN : ([], []),\n",
    "    DOC_PRESERVE : ([], []),\n",
    "    WORD_OCCUR: ([], [])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "he : dgl.DGLHeteroGraph = dgl.heterograph(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_idx, doc in enumerate(x_train):\n",
    "    dst_node = [doc_idx] * len(doc)\n",
    "    feature = []\n",
    "    for word_idx in doc:\n",
    "        feature.append(tfidfTable[doc_idx, word_idx - DIC_OFFSET])\n",
    "    he.add_edges(doc, dst_node,data = {'w': torch.tensor(feature, dtype=torch.float32)}, etype=DOC_CONTAIN)\n",
    "he = dgl.to_simple(he, copy_edata=True)\n",
    "he = dgl.add_self_loop(he, etype = DOC_PRESERVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge ('word', 'in', 'doc') : (tensor([    0,     0,     0,  ..., 19840, 19840, 19841]), tensor([   9,   17,   18,  ...,  782,  783, 2282]))\n",
      "Edge ('doc', 'preserve', 'doc') : (tensor([   0,    1,    2,  ..., 5482, 5483, 5484]), tensor([   0,    1,    2,  ..., 5482, 5483, 5484]))\n"
     ]
    }
   ],
   "source": [
    "print(f'Edge {DOC_CONTAIN} : {he.edges(etype = DOC_CONTAIN)}')\n",
    "print(f'Edge {DOC_PRESERVE} : {he.edges(etype = PRESERVE)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "he.edges[PRESERVE].data['w'] = torch.ones(size=(len(x_train),), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPMI\n",
    "WINDOW_SIZE = 3\n",
    "windows_list = []\n",
    "for sentence in x_train:\n",
    "    windows = [sentence[i : i + WINDOW_SIZE] for i in range(len(sentence) - WINDOW_SIZE + 1)]\n",
    "    windows_list += windows\n",
    "windows_list = np.array(windows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPMI_TABLE = {}\n",
    "def PPMI(a, b):\n",
    "    if a > b:\n",
    "        a, b = b, a\n",
    "    if (a, b) in PPMI_TABLE:\n",
    "        return PPMI_TABLE[(a,b)]\n",
    "    pa = ((windows_list==a).sum(axis = 1) != 0).sum().astype(np.float32) /  len(windows_list)\n",
    "    pb = ((windows_list==b).sum(axis = 1) != 0).sum().astype(np.float32) /  len(windows_list)\n",
    "    pab  = (((windows_list==a).sum(axis = 1) != 0) & ((windows_list==b).sum(axis = 1) != 0)).sum().astype(np.float32) / len(windows_list)\n",
    "    ppmi = np.log(np.exp(pab / pa * pb))\n",
    "    PPMI_TABLE[(a,b)] = ppmi\n",
    "    return PPMI_TABLE[(a,b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153580/153580 [36:47<00:00, 69.56it/s] \n"
     ]
    }
   ],
   "source": [
    "for window in tqdm(windows_list):\n",
    "    for i in range(len(window)):\n",
    "        for j in range(0, len(window)):\n",
    "            if not he.has_edges_between(window[i], window[j], etype=WORD_OCCUR):\n",
    "                if i == j:\n",
    "                    he.add_edges(window[i], window[j], data={'w':torch.tensor([1.], dtype=torch.float32)}, etype=WORD_OCCUR)\n",
    "                else:\n",
    "                    ppmi = PPMI(window[i], window[j])\n",
    "                    he.add_edges(window[i], window[j], data={'w':torch.tensor([ppmi], dtype=torch.float32)}, etype=WORD_OCCUR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl.save_graphs(filename='./R8/Transductive.bin', g_list = [he])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('torchEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "856971fa0a169429db2bf0c06b69517a5be6dea1d062d3d5bb82efdd1c5fddc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
