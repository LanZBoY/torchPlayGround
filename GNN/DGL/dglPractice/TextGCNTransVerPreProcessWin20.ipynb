{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wen2Tee5\\Desktop\\Postgraduate\\torchPlayGround\\torchEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from NLPUtils.preprocessUtils import removePunctuation, removeWord\n",
    "from NLPUtils.DataModel import Vocabulary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '.\\\\R8'\n",
    "TRAIN = 'train.txt'\n",
    "TEST = 'test.txt'\n",
    "MAXLEN = 30\n",
    "TOKEN = {'<PAD>' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeWordList = stopwords.words('english')\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(ROOT, TRAIN), encoding='utf-8',sep='\\t', header=None)\n",
    "df_test = pd.read_csv(os.path.join(ROOT, TEST), encoding='utf-8', sep = '\\t', header=None)\n",
    "label2idx = {label : i for i, label in enumerate(df_train[0].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = Vocabulary(TOKENS=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = []\n",
    "test_sentence = []\n",
    "for sentence in df_train[1]:\n",
    "    sentence = removePunctuation(sentence=sentence)\n",
    "    sentence = removeWord(removeWordList=removeWordList, sentence=sentence)\n",
    "    train_sentence.append(sentence)\n",
    "for sentence in df_test[1]:\n",
    "    sentence = removePunctuation(sentence=sentence)\n",
    "    sentence = removeWord(removeWordList=removeWordList, sentence=sentence)\n",
    "    test_sentence.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算tf-idf權重並建立詞彙庫\n",
    "vector = CountVectorizer(token_pattern=r'\\S+')\n",
    "table = vector.fit_transform(train_sentence + test_sentence)\n",
    "tfidf = TfidfTransformer()\n",
    "tfidfTable = tfidf.fit_transform(table)\n",
    "voc = Vocabulary(TOKENS=TOKEN)\n",
    "voc.addWordList(vector.get_feature_names_out().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將資料encode為idx\n",
    "train_idxs = []\n",
    "for sentence in train_sentence:\n",
    "    data = sentence.split()[:MAXLEN]\n",
    "    train_idx = [voc.word2idx[word] for word in data]\n",
    "    while len(train_idx) < MAXLEN:\n",
    "        train_idx.append(voc.word2idx['<PAD>'])\n",
    "    train_idxs.append(train_idx)\n",
    "\n",
    "test_idxs = []\n",
    "for sentence in test_sentence:\n",
    "    data = sentence.split()[:MAXLEN]\n",
    "    test_idx = []\n",
    "    for word in data:\n",
    "        if voc.has(word):\n",
    "            test_idx.append(voc.word2idx[word])\n",
    "        else:\n",
    "            test_idx.append(voc.word2idx['<PAD>'])\n",
    "    while len(test_idx) < MAXLEN:\n",
    "        test_idx.append(voc.word2idx['<PAD>'])\n",
    "    test_idxs.append(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train_idxs, dtype=np.int64)\n",
    "x_test = np.array(test_idxs, dtype=np.int64)\n",
    "# y_train = np.array([label2idx[label] for label in df_train[0]], dtype=np.int64)\n",
    "# y_test = np.array([label2idx[label] for label in df_test[0]], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('./R8/y_train.npy', y_train)\n",
    "# np.save('./R8/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC = 'doc'\n",
    "WORD = 'word'\n",
    "IN  = 'in'\n",
    "OCCUR = 'occur'\n",
    "PRESERVE = 'preserve'\n",
    "DOC_PRESERVE = (DOC, PRESERVE, DOC)\n",
    "DOC_CONTAIN = (WORD, IN, DOC)\n",
    "WORD_OCCUR = (WORD, OCCUR, WORD)\n",
    "DIC_OFFSET = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立好初始化的圖\n",
    "graph_data = {\n",
    "    DOC_CONTAIN : ([], []),\n",
    "    DOC_PRESERVE : ([], []),\n",
    "    WORD_OCCUR: ([], [])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "he : dgl.DGLHeteroGraph = dgl.heterograph(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_idx, doc in enumerate(x_train):\n",
    "    src_node = []\n",
    "    dst_node = []\n",
    "    feature = []\n",
    "    for word_idx in doc:\n",
    "        src_node.append(word_idx)\n",
    "        feature.append(tfidfTable[doc_idx, word_idx - DIC_OFFSET])\n",
    "        dst_node.append(doc_idx)\n",
    "    he.add_edges(src_node, dst_node,data = {'w': torch.tensor(feature, dtype=torch.float32)}, etype=DOC_CONTAIN)\n",
    "    \n",
    "for doc_idx, doc in enumerate(x_test, start=len(x_train)):\n",
    "    src_node = []\n",
    "    dst_node = []\n",
    "    feature = []\n",
    "    for word_idx in doc:\n",
    "        src_node.append(word_idx)\n",
    "        feature.append(tfidfTable[doc_idx, word_idx - DIC_OFFSET])\n",
    "        dst_node.append(doc_idx)\n",
    "            \n",
    "    he.add_edges(src_node, dst_node,data = {'w': torch.tensor(feature, dtype=torch.float32)}, etype=DOC_CONTAIN)\n",
    "he = dgl.to_simple(he, copy_edata=True)\n",
    "he = dgl.add_self_loop(he, etype = DOC_PRESERVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge ('word', 'in', 'doc') : (tensor([    0,     0,     0,  ..., 23439, 23439, 23440]), tensor([   9,   17,   18,  ...,  783, 5807, 2282]))\n",
      "Edge ('doc', 'preserve', 'doc') : (tensor([   0,    1,    2,  ..., 7671, 7672, 7673]), tensor([   0,    1,    2,  ..., 7671, 7672, 7673]))\n"
     ]
    }
   ],
   "source": [
    "print(f'Edge {DOC_CONTAIN} : {he.edges(etype = DOC_CONTAIN)}')\n",
    "print(f'Edge {DOC_PRESERVE} : {he.edges(etype = PRESERVE)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = []\n",
    "for i in range(len(x_train) + len(x_test)):\n",
    "    if i < len(x_train):\n",
    "        train_mask.append(True)\n",
    "    else:\n",
    "        train_mask.append(False)\n",
    "train_mask = torch.tensor(train_mask, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  ..., False, False, False])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "he.nodes[DOC].data['train_mask'] = train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5485, 30)\n",
      "(2189, 30)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3564, 16291,  3543, ..., 17069, 18914,  1027],\n",
       "       [ 4340, 20936, 20603, ...,  6221,  4235, 18274],\n",
       "       [ 4030, 10360,  3370, ..., 23330, 10374,  7565],\n",
       "       ...,\n",
       "       [10919, 19580, 21979, ..., 13503, 14107, 16725],\n",
       "       [13796, 23377,  3614, ...,  9866, 15300,  7099],\n",
       "       [11078, 10433,  4436, ..., 21094, 15300, 15300]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((x_train,x_test), axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "he.edges[PRESERVE].data['w'] = torch.ones(size=(len(np.concatenate((x_train,x_test), axis=0)),), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPMI\n",
    "WINDOW_SIZE = 20\n",
    "windows_list = []\n",
    "for sentence in np.concatenate((x_train, x_test), axis=0):\n",
    "    windows = [sentence[i : i + WINDOW_SIZE] for i in range(len(sentence) - WINDOW_SIZE + 1)]\n",
    "    windows_list += windows\n",
    "windows_list = np.array(windows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPMI_TABLE = {}\n",
    "HIT_TABLE = {}\n",
    "def PPMI(a, b):\n",
    "    if a > b:\n",
    "        a, b = b, a\n",
    "    try:\n",
    "        return PPMI_TABLE[(a,b)]\n",
    "    except:\n",
    "        if a == b:\n",
    "            return 1.\n",
    "        if a not in HIT_TABLE:\n",
    "            HIT_TABLE[a] = (windows_list==a).sum(axis = 1) != 0 \n",
    "        if b not in HIT_TABLE:\n",
    "            HIT_TABLE[b] = (windows_list==b).sum(axis = 1) != 0\n",
    "        hit_a = HIT_TABLE[a]\n",
    "        hit_b = HIT_TABLE[b]\n",
    "        pa = hit_a.sum().astype(np.float32) /  len(windows_list)\n",
    "        pb = hit_b.sum().astype(np.float32) /  len(windows_list)\n",
    "        pab = (hit_a & hit_b).sum().astype(np.float32) / len(windows_list)\n",
    "        ppmi = np.log(pab / pa * pb)\n",
    "        PPMI_TABLE[(a,b)] = ppmi\n",
    "        return PPMI_TABLE[(a,b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_nodes = []\n",
    "dst_nodes = []\n",
    "edge_feats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84414/84414 [02:34<00:00, 545.10it/s] \n"
     ]
    }
   ],
   "source": [
    "for window in tqdm(windows_list):\n",
    "    for i in range(len(window)):\n",
    "        for j in range(0, len(window)):\n",
    "            ppmi = PPMI(window[i], window[j])\n",
    "            src_nodes.append(window[i])\n",
    "            dst_nodes.append(window[j])\n",
    "            edge_feats.append(ppmi)\n",
    "he.add_edges(src_nodes, dst_nodes, data={'w':torch.tensor(edge_feats, dtype=torch.float32)}, etype=WORD_OCCUR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "he = dgl.to_simple(he, copy_edata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl.save_graphs(filename='./R8/TransductiveWin20.bin', g_list = [he])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('torchEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "856971fa0a169429db2bf0c06b69517a5be6dea1d062d3d5bb82efdd1c5fddc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
