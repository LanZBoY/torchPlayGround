{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from NLPUtils.preprocessUtils import removePunctuation, removeWord\n",
    "from NLPUtils.DataModel import Vocabulary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 30\n",
    "TOKEN = {'<PAD>' : 0}\n",
    "removeWordList = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./R8/train.txt', encoding='utf-8',sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = []\n",
    "for sentence in df[1]:\n",
    "    sentence = removePunctuation(sentence=sentence)\n",
    "    sentence = removeWord(removeWordList=removeWordList, sentence=sentence)\n",
    "    train_sentence.append(sentence)\n",
    "vector = CountVectorizer(token_pattern=r'\\S+')\n",
    "table = vector.fit_transform(train_sentence)\n",
    "tfidf = TfidfTransformer()\n",
    "tfidfTable = tfidf.fit_transform(table)\n",
    "voc = Vocabulary(TOKENS=TOKEN)\n",
    "voc.addWordList(vector.get_feature_names_out().tolist())\n",
    "\n",
    "train_idxs = []\n",
    "for sentence in train_sentence:\n",
    "    data = sentence.split()[:MAXLEN]\n",
    "    train_idx = [voc.word2idx[word] for word in data]\n",
    "    while len(train_idx) < MAXLEN:\n",
    "        train_idx.append(voc.word2idx['<PAD>'])\n",
    "    train_idxs.append(train_idx)\n",
    "\n",
    "\n",
    "x_train = np.array(train_idxs, dtype=np.int64)\n",
    "WINDOW_SIZE = 3\n",
    "windows_list = []\n",
    "for sentence in x_train:\n",
    "    windows = [sentence[i : i + WINDOW_SIZE] for i in range(len(sentence) - WINDOW_SIZE + 1)]\n",
    "    windows_list += windows\n",
    "windows_list = np.array(windows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPMI_TABLE = {}\n",
    "HIT_TABLE = {}\n",
    "def PPMI(a, b):\n",
    "    if a > b:\n",
    "        a, b = b, a\n",
    "    try:\n",
    "        return PPMI_TABLE[(a,b)]\n",
    "    except:\n",
    "        if a == b:\n",
    "            return 1.\n",
    "        if a not in HIT_TABLE:\n",
    "            HIT_TABLE[a] = (windows_list==a).sum(axis = 1) != 0 \n",
    "        if b not in HIT_TABLE:\n",
    "            HIT_TABLE[b] = (windows_list==b).sum(axis = 1) != 0\n",
    "        hit_a = HIT_TABLE[a]\n",
    "        hit_b = HIT_TABLE[b]\n",
    "        pa = hit_a.sum().astype(np.float32) /  len(windows_list)\n",
    "        pb = hit_b.sum().astype(np.float32) /  len(windows_list)\n",
    "        pab = (hit_a & hit_b).sum().astype(np.float32) / len(windows_list)\n",
    "        ppmi = np.log(pab / pa * pb)\n",
    "        PPMI_TABLE[(a,b)] = ppmi\n",
    "        return PPMI_TABLE[(a,b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153580/153580 [00:55<00:00, 2757.34it/s]\n",
      "100%|██████████| 244110/244110 [00:00<00:00, 3089282.25it/s]\n"
     ]
    }
   ],
   "source": [
    "wordNodes = {}\n",
    "for window in tqdm(windows_list):\n",
    "    for i in range(len(window)):\n",
    "        for j in range(0, len(window)):\n",
    "            if((window[i], window[j]) not in wordNodes):\n",
    "                ppmi = PPMI(window[i], window[j])\n",
    "                wordNodes[ (window[i], window[j]) ] = ppmi\n",
    "src_nodes = []\n",
    "dst_nodes = []\n",
    "edge_feats = []\n",
    "for (u, v), w in tqdm(wordNodes.items()):\n",
    "    src_nodes.append(u)\n",
    "    dst_nodes.append(v)\n",
    "    edge_feats.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153580/153580 [00:01<00:00, 128168.02it/s]\n",
      "100%|██████████| 244110/244110 [00:00<00:00, 2068264.42it/s]\n"
     ]
    }
   ],
   "source": [
    "wordNodes = set()\n",
    "for window in tqdm(windows_list):\n",
    "    for i in range(len(window)):\n",
    "        for j in range(0, len(window)):\n",
    "            ppmi = PPMI(window[i], window[j])\n",
    "            wordNodes.add((window[i], window[j], ppmi))\n",
    "src_nodes = []\n",
    "dst_nodes = []\n",
    "edge_feats = []\n",
    "for u, v, w in tqdm(wordNodes):\n",
    "    src_nodes.append(u)\n",
    "    dst_nodes.append(v)\n",
    "    edge_feats.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('torchEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "856971fa0a169429db2bf0c06b69517a5be6dea1d062d3d5bb82efdd1c5fddc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
