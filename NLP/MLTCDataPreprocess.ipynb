{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from NLPUtils.DataModel import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./MLTCDataset/RCV1/data/topic_sorted.json') as f:\n",
    "    hash_tgt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_voc = Vocabulary({\"<UNK>\":0}, MAXLEN = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructing Voc: 100%|██████████| 775220/775220 [00:30<00:00, 25449.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# load Training Data\n",
    "with open('./MLTCDataset/RCV1/data/train.src') as f:\n",
    "    src_datas = f.read().split('\\n')[:-1]\n",
    "train_x = []\n",
    "for sentence in tqdm(src_datas, desc='Constructing Voc'):\n",
    "    sentence = train_voc.addSentence(sentence = sentence)\n",
    "    idx_data = [train_voc.word2idx[word] for word in sentence]\n",
    "    while (len(idx_data) < train_voc.MAXLEN):\n",
    "        idx_data.append(train_voc.word2idx['<UNK>'])\n",
    "    train_x.append(idx_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./MLTCDataset/RCV1/data/train.tgt') as f:\n",
    "    tgt_datas = f.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "transfer tag: 100%|██████████| 775220/775220 [00:00<00:00, 836476.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: need to translate to sparse matrix vector\n",
    "train_y = []\n",
    "for tgts in tqdm(tgt_datas, desc='transfer tag'):\n",
    "    tgts = tgts.split()\n",
    "    tgt_vector = np.zeros(shape=(len(hash_tgt)), dtype=np.float32)\n",
    "    for tgt in tgts:\n",
    "        tgt_vector[hash_tgt[tgt]] = 1.\n",
    "    train_y.append(tgt_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./MLTCDataset/RCV1/train_voc.data', 'wb') as f :\n",
    "    pickle.dump(train_voc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x, dtype=np.int64)\n",
    "train_y = np.array(train_y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./MLTCDataset/RCV1/data/train_x.npy', train_x)\n",
    "np.save('./MLTCDataset/RCV1/data/train_y.npy', train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test to word index: 100%|██████████| 1191/1191 [00:00<00:00, 25047.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# test Xset\n",
    "with open('./MLTCDataset/RCV1/data/test.src') as f:\n",
    "    src_datas = f.read().split('\\n')[:-1]\n",
    "test_x = []\n",
    "for sentence in tqdm(src_datas, desc='test to word index'):\n",
    "    sentence = sentence.split()[:train_voc.MAXLEN]\n",
    "    idx_data = []\n",
    "    for word in sentence:\n",
    "        if train_voc.has(word):\n",
    "            idx_data.append(train_voc.word2idx[word])\n",
    "        else:\n",
    "            idx_data.append(train_voc.word2idx[\"<UNK>\"])\n",
    "    while (len(idx_data) < train_voc.MAXLEN):\n",
    "        idx_data.append(train_voc.word2idx['<UNK>'])\n",
    "    test_x.append(idx_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "transfer tag: 100%|██████████| 1191/1191 [00:00<00:00, 595401.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# test Yset\n",
    "with open('./MLTCDataset/RCV1/data/test.tgt') as f:\n",
    "    tgt_datas = f.read().split('\\n')[:-1]\n",
    "test_y = []\n",
    "for tgts in tqdm(tgt_datas, desc='transfer tag'):\n",
    "    tgts = tgts.split()\n",
    "    tgt_vector = np.zeros(shape=(len(hash_tgt)), dtype=np.float32)\n",
    "    for tgt in tgts:\n",
    "        tgt_vector[hash_tgt[tgt]] = 1.\n",
    "    test_y.append(tgt_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.array(test_x, dtype=np.int64)\n",
    "test_y = np.array(test_y, dtype=np.float32)\n",
    "np.save('./MLTCDataset/RCV1/data/test_x.npy', test_x)\n",
    "np.save('./MLTCDataset/RCV1/data/test_y.npy', test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid to word index: 100%|██████████| 21510/21510 [00:00<00:00, 23329.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# valid Xset\n",
    "with open('./MLTCDataset/RCV1/data/valid.src') as f:\n",
    "    src_datas = f.read().split('\\n')[:-1]\n",
    "valid_x = []\n",
    "for sentence in tqdm(src_datas, desc='valid to word index'):\n",
    "    sentence = sentence.split()[:train_voc.MAXLEN]\n",
    "    idx_data = []\n",
    "    for word in sentence:\n",
    "        if train_voc.has(word):\n",
    "            idx_data.append(train_voc.word2idx[word])\n",
    "        else:\n",
    "            idx_data.append(train_voc.word2idx[\"<UNK>\"])\n",
    "    while (len(idx_data) < train_voc.MAXLEN):\n",
    "        idx_data.append(train_voc.word2idx['<UNK>'])\n",
    "    valid_x.append(idx_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "transfer tag: 100%|██████████| 21510/21510 [00:00<00:00, 660769.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# valid Yset\n",
    "with open('./MLTCDataset/RCV1/data/valid.tgt') as f:\n",
    "    tgt_datas = f.read().split('\\n')[:-1]\n",
    "valid_y = []\n",
    "for tgts in tqdm(tgt_datas, desc='transfer tag'):\n",
    "    tgts = tgts.split()\n",
    "    tgt_vector = np.zeros(shape=(len(hash_tgt)), dtype=np.float32)\n",
    "    for tgt in tgts:\n",
    "        tgt_vector[hash_tgt[tgt]] = 1.\n",
    "    valid_y.append(tgt_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = np.array(valid_x, dtype=np.int64)\n",
    "valid_y = np.array(valid_y, dtype=np.float32)\n",
    "np.save('./MLTCDataset/RCV1/data/valid_x.npy', valid_x)\n",
    "np.save('./MLTCDataset/RCV1/data/valid_y.npy', valid_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('torchEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "856971fa0a169429db2bf0c06b69517a5be6dea1d062d3d5bb82efdd1c5fddc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
